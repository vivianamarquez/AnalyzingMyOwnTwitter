{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import regex\n",
    "import json\n",
    "import emoji\n",
    "import emojis\n",
    "import humanize\n",
    "import calendar\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from collections import Counter\n",
    "\n",
    "# from IPython.core.interactiveshell import InteractiveShell\n",
    "# InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "import PIL.Image\n",
    "from IPython.display import Image\n",
    "import plotly.io as pio\n",
    "import plotly.express as px\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "from wordcloud import ImageColorGenerator\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "from nltk import pos_tag\n",
    "from nltk.util import ngrams \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "import pyLDAvis.gensim\n",
    "from gensim import corpora\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS as stopwords_en_spacy\n",
    "from spacy.lang.es.stop_words import STOP_WORDS as stopwords_sp_spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load my tweets provided by Twitter itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('twitter/data/tweet.js') as f:\n",
    "    file = f.read()\n",
    "    file = file[file.find(\"[\"):]\n",
    "    \n",
    "data = json.loads(file)\n",
    "print(f\"{len(data):,.0f} tweets\")\n",
    "df = json_normalize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove RTs\n",
    "\n",
    "df['RTs'] = df['tweet.full_text'].apply(lambda text: text.startswith(\"RT @\"))\n",
    "RTs = df[df['RTs']]['tweet.full_text'].apply(lambda text: text.split(\":\")[0]).values\n",
    "\n",
    "print(df['RTs'].value_counts())\n",
    "df = df[~df['RTs']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['tweet.id_str',\n",
    "         'tweet.retweeted', \n",
    "         'tweet.favorited',\n",
    "         'tweet.truncated', \n",
    "         'tweet.source', \n",
    "         'tweet.possibly_sensitive', \n",
    "         'tweet.entities.symbols',\n",
    "         'tweet.extended_entities.media',\n",
    "         'tweet.display_text_range', \n",
    "         'tweet.in_reply_to_status_id',\n",
    "         'tweet.in_reply_to_status_id_str', \n",
    "         'tweet.in_reply_to_user_id', \n",
    "         'tweet.in_reply_to_user_id_str',\n",
    "         'tweet.coordinates.type', \n",
    "         'tweet.coordinates.coordinates', \n",
    "         'tweet.geo.type', \n",
    "         'tweet.geo.coordinates',\n",
    "         'RTs'\n",
    "          ], axis=1, inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_urls(obj):\n",
    "    urls = []\n",
    "    if len(obj)==0:\n",
    "        return urls\n",
    "    for item in obj:\n",
    "        urls.append(item['expanded_url'])\n",
    "    return urls\n",
    "\n",
    "df['urls'] = df['tweet.entities.urls'].apply(lambda obj: get_urls(obj))\n",
    "df.drop(['tweet.entities.urls'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mentions(obj):\n",
    "    mentions = []\n",
    "    if len(obj)==0:\n",
    "        return mentions\n",
    "    for item in obj:\n",
    "        mentions.append(item['screen_name'])\n",
    "    return mentions\n",
    "\n",
    "df['mentions'] = df['tweet.entities.user_mentions'].apply(lambda obj: get_mentions(obj))\n",
    "df.drop(['tweet.entities.user_mentions'], axis=1, inplace=True)\n",
    "\n",
    "def remove_replies(mentions, replied_to):\n",
    "    try:\n",
    "        mentions.remove(replied_to)\n",
    "        return mentions\n",
    "    except:\n",
    "        return mentions\n",
    "\n",
    "df['mentions'] = df.apply(lambda x: remove_replies(x.mentions, x['tweet.in_reply_to_screen_name']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hashtags(obj):\n",
    "    hashtags = []\n",
    "    if len(obj)==0:\n",
    "        return hashtags\n",
    "    for item in obj:\n",
    "        hashtags.append(item['text'])\n",
    "    return hashtags\n",
    "\n",
    "df['hashtags'] = df['tweet.entities.hashtags'].apply(lambda obj: get_hashtags(obj))\n",
    "df.drop(['tweet.entities.hashtags'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_media_urls(obj):\n",
    "    media_urls = []\n",
    "    try:\n",
    "        if len(obj)==0:\n",
    "            return media_urls\n",
    "    except:\n",
    "        return media_urls\n",
    "    for item in obj:\n",
    "        media_urls.append(item['expanded_url'])\n",
    "    return media_urls\n",
    "\n",
    "df['media.urls'] = df['tweet.entities.media'].apply(lambda obj: get_media_urls(obj))\n",
    "df.drop(['tweet.entities.media'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['tweet.id',\n",
    "         'tweet.created_at',\n",
    "         'tweet.full_text',\n",
    "         'tweet.lang',\n",
    "         'tweet.retweet_count',\n",
    "         'tweet.favorite_count',\n",
    "         'mentions',\n",
    "         'tweet.in_reply_to_screen_name',\n",
    "         'hashtags',\n",
    "         'urls',\n",
    "         'media.urls'\n",
    "         ]]\n",
    "\n",
    "df.columns = ['id', 'date', 'text', 'lang',\n",
    "              'retweets', 'favorites', 'mentions', 'replied_to',\n",
    "              'hashtags', 'urls', 'media_urls']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = pd.to_datetime(df.date)\n",
    "df['day'] = df.date.dt.date\n",
    "df['YYYYMM'] = df.date.dt.to_period('M')\n",
    "df['year'] = df.date.dt.year\n",
    "df['dow'] = df.date.dt.day_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emojis(text):\n",
    "    emoji_list = []\n",
    "    data = regex.findall(r'\\X', text)\n",
    "    for word in data:\n",
    "        if any(char in emoji.UNICODE_EMOJI for char in word):\n",
    "            emoji_list.append(word)\n",
    "\n",
    "    return emoji_list\n",
    "\n",
    "df['emojis'] = df['text'].apply(lambda text: get_emojis(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern1 = '?P<pic>pic.twitter.com/[^\\s]+'\n",
    "pattern2 = '?P<url>https?://[^\\s]+'\n",
    "\n",
    "def text_clean(text):\n",
    "    links = [tuple(j for j in i if j)[-1] for i in re.findall(f\"({pattern1})|({pattern2})\",text)]\n",
    "    for link in links:\n",
    "        text = text.replace(link,\"\")\n",
    "             \n",
    "    hashtags = [interaction for interaction in text.split() if interaction.startswith(\"#\")]\n",
    "    for hashtag in hashtags:\n",
    "        text = text.replace(hashtag,\"\")\n",
    "        \n",
    "    mentions = [interaction for interaction in text.split() if interaction.startswith(\"@\")]\n",
    "    for mention in mentions:\n",
    "        text = text.replace(mention,\"\")\n",
    "        \n",
    "    return text\n",
    "\n",
    "df['text_clean'] = df['text'].apply(lambda text: text_clean(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['id_url'] = df['id'].apply(lambda text: \"https://mobile.twitter.com/user/status/\"+text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_color = \"#ff00a7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Time on Twitter with this account:\")\n",
    "print(f\"Since {humanize.naturaldate(df.date.min())}\")\n",
    "relativedelta(df.date.max(), df.date.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Day\n",
    "temp_day = pd.DataFrame(df.day.value_counts())\n",
    "temp_day.reset_index(inplace=True)\n",
    "temp_day.columns = ['day', 'tweet_count']\n",
    "temp_day.sort_values(by=['day'], inplace=True)\n",
    "temp_day.reset_index(inplace=True, drop=True)\n",
    "idx = pd.date_range(temp_day.day.min(), temp_day.day.max())\n",
    "temp_day.index = pd.DatetimeIndex(temp_day.day)\n",
    "temp_day = temp_day.reindex(idx, fill_value=0)\n",
    "temp_day.day = temp_day.index\n",
    "temp_day.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "trace_day = go.Scatter(\n",
    "                    x=temp_day.day.astype(str).values,\n",
    "                    y=temp_day.tweet_count.values,\n",
    "                    text = [f\"{humanize.naturaldate(day)}: {count} tweets\" for day,count in zip(temp_day.day.dt.date.values,temp_day.tweet_count.values)],\n",
    "                    hoverinfo='text',\n",
    "                    mode='lines', \n",
    "                    line = {\n",
    "                        'color': my_color,\n",
    "                        'width': 1.2\n",
    "                    },\n",
    "                    visible=False,\n",
    "                    name=\"Day\"\n",
    "                )\n",
    "\n",
    "# Plot Month\n",
    "temp_month = temp_day\n",
    "temp_month.index = pd.DatetimeIndex(temp_month.day)\n",
    "temp_month = temp_month.groupby(pd.Grouper(freq='M')).sum().reset_index()\n",
    "idx = pd.date_range(temp_month.day.min(), temp_month.day.max())\n",
    "temp_month.index = pd.DatetimeIndex(temp_month.day)\n",
    "temp_month = temp_month.reindex(idx, fill_value=0)\n",
    "temp_month.day = temp_month.index\n",
    "temp_month.reset_index(drop=True, inplace=True)\n",
    "temp_month['YYYYMM'] = temp_month['day'].dt.to_period('M')\n",
    "temp_month.drop_duplicates(subset=['YYYYMM'], keep='last', inplace=True)\n",
    "\n",
    "\n",
    "trace_month = go.Scatter(\n",
    "                    x=temp_month.day.astype('datetime64[M]').astype(str).values,\n",
    "                    y=temp_month.tweet_count.values,\n",
    "                    text = [f\"{humanize.naturaldate(month)[:3]+humanize.naturaldate(month)[6:]}: {count} tweets\" for month,count in zip(temp_month.YYYYMM.values,temp_month.tweet_count.values)],\n",
    "                    hoverinfo='text',\n",
    "                    mode='lines', \n",
    "                    line = {\n",
    "                        'color': my_color,\n",
    "                        'width': 1.2\n",
    "                    },\n",
    "                    visible=False,\n",
    "                    name=\"Month\"\n",
    "                ) \n",
    "\n",
    "# Plot year\n",
    "temp_year = temp_day\n",
    "temp_year.index = pd.DatetimeIndex(temp_year.day)\n",
    "temp_year = temp_year.groupby(pd.Grouper(freq='Y')).sum().reset_index()\n",
    "temp_year['year'] = temp_year.day.dt.year\n",
    "temp_year\n",
    "\n",
    "trace_year = go.Scatter(\n",
    "                    x=temp_year.day.astype('datetime64[Y]').astype(str).values,\n",
    "                    y=temp_year.tweet_count.values,\n",
    "                    text = [f\"Year {year}: {count:,.0f} tweets\" for year,count in zip(temp_year.year.values,temp_year.tweet_count.values)],\n",
    "                    hoverinfo='text',\n",
    "                    mode='lines+markers', \n",
    "                    line = {\n",
    "                        'color': my_color,\n",
    "                        'width': 1.2\n",
    "                    },\n",
    "                    visible=True,\n",
    "                    name=\"Year\"\n",
    "                ) \n",
    "\n",
    "\n",
    "# Menus\n",
    "updatemenus = list([\n",
    "    dict(\n",
    "         active=0,\n",
    "         buttons=list([\n",
    "            dict(label = 'Year',\n",
    "                 method = 'update',\n",
    "                 args = [{'visible': [True, False, False]},\n",
    "                         {'title': 'Number of Tweets per Year'}]),\n",
    "            dict(label = 'Month',\n",
    "                 method = 'update',\n",
    "                 args = [{'visible': [False, True, False]},\n",
    "                         {'title': 'Number of Tweets per Month'}]),\n",
    "            dict(label = 'Day',\n",
    "                 method = 'update',\n",
    "                 args = [{'visible': [False, False, True]},\n",
    "                         {'title': 'Number of Tweets per Day'}]),\n",
    "             ]),\n",
    "        \n",
    "    )\n",
    "])\n",
    "\n",
    "# Layout\n",
    "layout = go.Layout(title=\"Number of Tweets -- Pick a scale\",\n",
    "                   updatemenus=updatemenus,\n",
    "                  )\n",
    "\n",
    "fig = go.Figure(data=[trace_year, trace_month, trace_day], layout=layout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.DataFrame(df.groupby('day').count()['date'])\n",
    "temp.reset_index(inplace=True)\n",
    "temp.columns = ['date', 'tweet_count']\n",
    "temp['date_human'] = temp['date'].apply(lambda value: humanize.naturaldate(value))\n",
    "print(\"Days I tweeted the most:\")\n",
    "temp[['date_human', 'tweet_count']].sort_values('tweet_count', ascending=False)[:5].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp['date'] = pd.to_datetime(temp['date'])\n",
    "temp = temp[temp['date'].dt.year > 2012]\n",
    "temp['diff'] = temp['date'].diff().dt.days\n",
    "temp['date'] = temp['date'].apply(lambda value: humanize.naturaldate(value))\n",
    "print(\"Longest streaks without tweeting:\")\n",
    "temp[['date_human', 'diff']].sort_values('diff', ascending=False)[:5].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.DataFrame(df['lang'].value_counts()).reset_index()\n",
    "temp.columns = ['language', 'tweet_count']\n",
    "\n",
    "lang_dict = {'es': 'Spanish',\n",
    "             'en': 'English',\n",
    "             'pt': 'Portuguese',\n",
    "             'ru': 'Russian',\n",
    "             'fr': 'French',\n",
    "             'ja': 'Japanese'}\n",
    "\n",
    "# Others will include: mixed, one-word, one-emoji, missclasified, and otherss\n",
    "\n",
    "temp['language'] = temp['language'].apply(lambda lg: lang_dict[lg] if lg in lang_dict.keys() else 'Others')\n",
    "temp = temp.groupby('language').sum().sort_values('tweet_count', ascending=False).reset_index()\n",
    "\n",
    "# Plot\n",
    "trace = go.Bar(\n",
    "                x=temp.language.values,\n",
    "                y=temp.tweet_count.values,\n",
    "                opacity=0.6,\n",
    "                marker=dict(color=my_color,\n",
    "                            line=dict(color=my_color,width=1.5,)\n",
    "                           ),\n",
    "                )\n",
    "\n",
    "layout = go.Layout(title='Number of tweets per language<br> ALL TIME',\n",
    "                   yaxis=dict(tickformat=','))\n",
    "\n",
    "fig = go.Figure(data=[trace], layout=layout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traces = []\n",
    "\n",
    "lang_color_dict = {'Spanish': \"#f4ce26\",\n",
    "                   'English': \"#2093dd\",\n",
    "                   'Portuguese': \"#037f3e\",\n",
    "                   'Russian': \"#2135a3\",\n",
    "                   'French': \"#dd3228\",\n",
    "                   'Japanese': \"#b666de\"}\n",
    "\n",
    "temp = df[['lang', 'YYYYMM']]\n",
    "temp['language'] = temp['lang'].apply(lambda lg: lang_dict[lg] if lg in lang_dict.keys() else 'Others')\n",
    "temp = temp[temp['language']!='Others']\n",
    "temp['YYYYMM'] = temp['YYYYMM'].dt.strftime('%Y-%m')\n",
    "temp = temp.groupby(['language', 'YYYYMM'])['language'].count()\n",
    "temp = pd.DataFrame(temp / temp.groupby(level=1).sum())\n",
    "temp.columns = ['pct']\n",
    "temp.reset_index(inplace=True)\n",
    "\n",
    "for lang in temp['language'].unique():\n",
    "    trace= go.Scatter(\n",
    "                        x=temp[temp['language']==lang].YYYYMM.values,\n",
    "                        y=temp[temp['language']==lang].pct.values,\n",
    "                        mode='lines', \n",
    "                        line = {\n",
    "                            'color': lang_color_dict[lang],\n",
    "                            'width': 1.2\n",
    "                        },\n",
    "                        name=lang\n",
    "                    ) \n",
    "\n",
    "    traces.append(trace)\n",
    "    \n",
    "layout = go.Layout(title='Percentage of tweets per language',\n",
    "                   yaxis=dict(tickformat=',.0%'))\n",
    "\n",
    "fig = go.Figure(data=traces, layout=layout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "df.retweets = pd.to_numeric(df.retweets)\n",
    "print(f\"Max number of retweets: {df.retweets.max()}\")\n",
    "\n",
    "df[['day', 'text_clean', 'retweets', 'favorites', 'id_url']].sort_values('retweets', ascending=False).head(5).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "df.favorites = pd.to_numeric(df.favorites)\n",
    "print(f\"Max number of favorites: {df.favorites.max()}\")\n",
    "\n",
    "df[['day', 'text_clean', 'retweets', 'favorites', 'id_url']].sort_values('favorites', ascending=False).head(5).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(sum(df['mentions'].values,[])).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['replied_to'].value_counts()[:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(RTs).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total tweets: 16,534\")\n",
    "print(f\"From which...\")\n",
    "print(f\"5,957 are re-tweets\")\n",
    "print(f\"{df['replied_to'].isna().value_counts()[False]:,.0f} are replies\")\n",
    "print(f\"{sum(df['urls'].apply(lambda value: True if len(value)!=0 else False) | df['media_urls'].apply(lambda value: True if len(value)!=0 else False)):,.0f} have media\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(sum(df['urls'].values,[])).most_common(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " HASHTAGS (Tableau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_year = []\n",
    "\n",
    "for year in list(range(2011,2021)):\n",
    "    temp = pd.DataFrame(Counter(sum(df[df['year']==year]['hashtags'].values,[])).most_common(30))\n",
    "    temp['year'] = year\n",
    "    temp_year.append(temp)\n",
    "    \n",
    "temp_year = pd.concat(temp_year)\n",
    "temp_year.columns = ['hashtag', 'tweet_count', 'year']\n",
    "\n",
    "temp_year.to_csv(\"hashtag.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vader = SentimentIntensityAnalyzer() \n",
    "\n",
    "def transform_emoji(es):\n",
    "    if len(es)==0:\n",
    "        return []\n",
    "    \n",
    "    result = []\n",
    "    for e in es:\n",
    "        e_string = emoji.demojize(e)[1:-1].replace(\"_\",\" \")\n",
    "        e_list = [(emoji, name) for emoji, name in vader.emojis.items() if e_string==name]\n",
    "        if len(e_list)==0:\n",
    "            result.append(e)\n",
    "            continue\n",
    "        result.append(e_list[0][0])\n",
    "    return result\n",
    "\n",
    "df['emojis'] = df['emojis'].apply(lambda es: transform_emoji(es))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All time\n",
    "print(f\"{sum(list(Counter(sum(df['emojis'].values,[])).values())):,.0f} emojis\")\n",
    "print(f\"{len(Counter(sum(df['emojis'].values,[])).most_common())} different ones\\n\\n\")\n",
    "      \n",
    "for i, em in enumerate(Counter(sum(df['emojis'].values,[])).most_common()):\n",
    "    e = em[0]\n",
    "    print(e)\n",
    "    print(f\"Rank #{i+1}\")\n",
    "    print(f\"Count: {em[1]}\")\n",
    "    try:\n",
    "        print(f\"Sentiment: {vader.polarity_scores(e)['compound']}\")\n",
    "        print(f\"Category: {emojis.db.get_emoji_by_code(e).category}\")\n",
    "        print()\n",
    "    except:\n",
    "        print(\"NA************************************************\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per year\n",
    "temp_emoji = []\n",
    "\n",
    "for year in list(range(2011,2021)):\n",
    "    temp = pd.DataFrame(Counter(sum(df[df['year']==year]['emojis'].values,[])).most_common(10))\n",
    "    temp['year'] = year\n",
    "    temp_emoji.append(temp)\n",
    "    \n",
    "temp_emoji = pd.concat(temp_emoji)\n",
    "temp_emoji.columns = ['emojis', 'tweet_count', 'year']\n",
    "\n",
    "temp_emoji = temp_emoji.pivot(index='emojis', columns='year').reset_index()\n",
    "temp_emoji.columns = ['emojis'] + list(temp_emoji.columns.get_level_values(1)[1:])\n",
    "\n",
    "# I hard-coded the colors because all emoji libraries out there suck!\n",
    "gray = '#d2d2d0' # Others\n",
    "green = '#9ae397' # Nature\n",
    "yellow = '#fbf690' # Happy\n",
    "orange = '#ffbe6d' # Travel\n",
    "pink = '#ff6dbe' # Hearts\n",
    "blue = '#6dbeff' # Sad\n",
    "\n",
    "emojis_colors = [gray, green, green, yellow, pink, gray, orange, yellow, yellow, green, \n",
    "                 pink, orange, orange, gray, yellow, yellow, yellow, pink, pink, pink, \n",
    "                 pink, yellow, yellow, yellow, yellow, yellow, yellow, yellow, yellow, yellow, \n",
    "                 blue, blue, blue, blue, blue, yellow, blue, yellow, blue]\n",
    "\n",
    "temp_emoji['colors'] = emojis_colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beg = 2014\n",
    "listOfFrames = []\n",
    "for year in range(beg,2021):\n",
    "    temp = temp_emoji[['emojis', year, 'colors']]\n",
    "    temp = temp.sort_values(year, ascending=False).dropna()\n",
    "    \n",
    "    trace = go.Bar(\n",
    "                x=temp.emojis.values,\n",
    "                y=temp[year].values,\n",
    "                textposition = \"outside\",\n",
    "                texttemplate = \"%{x}<br>%{y}\",\n",
    "                textfont = {'size': [14]*10},\n",
    "                hoverinfo='none', \n",
    "                opacity=1,\n",
    "                marker=dict(color=temp.colors.values,\n",
    "                           ),\n",
    "                cliponaxis = False\n",
    "                )\n",
    "\n",
    "    layout = go.Layout(title=f'Emojis in {year}',\n",
    "                       xaxis = {\"range\":[-1,10],\"autorange\": False, \"showline\":False,\"tickangle\":-90, \"visible\":False},\n",
    "                       yaxis = {\"range\":[-1,200],\"autorange\": False, \"showline\":False, \"visible\":False},\n",
    "                       plot_bgcolor= '#FFFFFF')\n",
    "    listOfFrames.append(go.Frame(data=[trace], layout=layout))\n",
    "    \n",
    "frames=list(listOfFrames)\n",
    "\n",
    "year = beg\n",
    "temp = temp_emoji[['emojis', year, 'colors']]\n",
    "temp = temp.sort_values(year, ascending=False).dropna()\n",
    "\n",
    "# Plot\n",
    "trace = go.Bar(\n",
    "                x=temp.emojis.values,\n",
    "                y=temp[year].values,\n",
    "                textposition = \"outside\",\n",
    "                texttemplate = \"%{x}<br>%{y}\",\n",
    "                textfont = {'size': [14]*10},\n",
    "                hoverinfo='none', \n",
    "                opacity=1,\n",
    "                marker=dict(color=temp.colors.values,\n",
    "                           ),\n",
    "                cliponaxis = False\n",
    "                )\n",
    "\n",
    "layout = go.Layout(title=f'Emojis in {year}',\n",
    "                   xaxis = {\"range\":[-1,10],\"autorange\": False, \"showline\":False,\"tickangle\":-90, \"visible\":False},\n",
    "                   yaxis = {\"range\":[-1,200],\"autorange\": False, \"showline\":False, \"visible\":False},\n",
    "                   plot_bgcolor= '#FFFFFF',\n",
    "                   updatemenus=[dict(type=\"buttons\",\n",
    "                                     buttons=[dict(label=\"▶️\",\n",
    "                                                   method=\"animate\",\n",
    "                                                   args=[None, {\"frame\": {\"duration\": 800, \"redraw\": True},\n",
    "                                                                \"fromcurrent\": True}]),\n",
    "                                              {\n",
    "                \"args\": [[None], {\"frame\": {\"duration\": 0, \"redraw\": True},\n",
    "                                  \"mode\": \"immediate\",\n",
    "                                  \"transition\": {\"duration\": 0}}],\n",
    "                \"label\": \"⏸️\",\n",
    "                \"method\": \"animate\"\n",
    "            }\n",
    "                                             ]\n",
    "                                    )\n",
    "                               ]\n",
    "                  )\n",
    "\n",
    "fig = go.Figure(data=[trace], layout=layout, frames=frames)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per month\n",
    "temp_emoji = []\n",
    "\n",
    "for ym in sorted(df.YYYYMM.unique()):\n",
    "    temp = pd.DataFrame(Counter(sum(df[df['YYYYMM']==ym]['emojis'].values,[])).most_common(5))\n",
    "    temp['YYYYMM'] = ym\n",
    "    if temp.shape[0] != 0:\n",
    "        temp_emoji.append(temp)\n",
    "    \n",
    "temp_emoji = pd.concat(temp_emoji)\n",
    "temp_emoji.columns = ['emojis', 'tweet_count', 'YYYYMM']\n",
    "\n",
    "temp_emoji = temp_emoji.pivot(index='emojis', columns='YYYYMM').reset_index()\n",
    "temp_emoji.columns = ['emojis'] + list(temp_emoji.columns.get_level_values(1)[1:])\n",
    "\n",
    "# I hard-coded the colors because all emoji libraries out there suck!\n",
    "gray = '#d2d2d0' # Others\n",
    "green = '#9ae397' # Nature\n",
    "yellow = '#fbf690' # Happy\n",
    "orange = '#ffbe6d' # Travel\n",
    "pink = '#ff6dbe' # Hearts\n",
    "blue = '#6dbeff' # Sad\n",
    "purple = \"#be6dff\" # Celebration\n",
    "\n",
    "emojis_colors = [gray, green, green, gray, blue, yellow, pink, gray, orange, yellow, \n",
    "                 yellow, green, green, pink, orange, orange, orange, orange, orange, green, \n",
    "                 green, green, green, purple, purple, purple, purple, purple, gray, gray, \n",
    "                 green, gray, gray, yellow, yellow, yellow, gray, pink, pink, pink, \n",
    "                 pink, pink, gray, yellow, gray, gray, yellow, yellow, yellow, yellow, \n",
    "                 yellow, yellow, yellow, yellow, yellow, blue, blue, blue, blue, yellow, \n",
    "                 blue, blue, blue, blue, blue, blue, blue, blue, blue, blue, \n",
    "                 yellow, blue, yellow, blue, yellow, yellow, yellow, yellow, yellow, yellow, \n",
    "                 gray, blue, gray, yellow, yellow, yellow, blue, yellow, blue, green]\n",
    "\n",
    "temp_emoji['colors'] = emojis_colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beg = temp_emoji.columns[13]\n",
    "listOfFrames = []\n",
    "for year in temp_emoji.columns[1:-1]:\n",
    "    temp = temp_emoji[['emojis', year, 'colors']]\n",
    "    temp = temp.sort_values(year, ascending=True).dropna()\n",
    "    if temp.shape[0]!=5:\n",
    "        continue\n",
    "    \n",
    "    trace = go.Bar(\n",
    "                y=temp.emojis.values,\n",
    "                x=temp[year].values,\n",
    "                orientation='h',\n",
    "                textposition = \"outside\",\n",
    "                texttemplate = \"%{x}\",\n",
    "                textfont = {'size': [10]*10},\n",
    "                hoverinfo='none', \n",
    "                opacity=1,\n",
    "                marker=dict(color=temp.colors.values,\n",
    "                           ),\n",
    "                cliponaxis = False\n",
    "                )\n",
    "\n",
    "    layout = go.Layout(title=f'Emojis in {calendar.month_name[humanize.naturaltime(year).month]} {humanize.naturaltime(year).year}',\n",
    "                       yaxis = {\"range\":[-1,5],\"autorange\": False, \"showline\":False, \"visible\":True},\n",
    "                       xaxis = {\"range\":[-1,150],\"autorange\": False, \"showline\":False, \"visible\":False},\n",
    "                       plot_bgcolor= '#FFFFFF')\n",
    "    listOfFrames.append(go.Frame(data=[trace], layout=layout))\n",
    "    \n",
    "frames=list(listOfFrames)\n",
    "\n",
    "year = beg\n",
    "temp = temp_emoji[['emojis', year, 'colors']]\n",
    "temp = temp.sort_values(year, ascending=True).dropna()\n",
    "\n",
    "# Plot\n",
    "trace = go.Bar(\n",
    "                y=temp.emojis.values,\n",
    "                x=temp[year].values,\n",
    "                orientation='h',\n",
    "                textposition = \"outside\",\n",
    "                texttemplate = \"%{x}\",\n",
    "                textfont = {'size': [10]*10},\n",
    "                hoverinfo='none', \n",
    "                opacity=1,\n",
    "                marker=dict(color=temp.colors.values,\n",
    "                           ),\n",
    "                cliponaxis = False\n",
    "                )\n",
    "\n",
    "layout = go.Layout(title=f'Emojis in {calendar.month_name[humanize.naturaltime(year).month]} {humanize.naturaltime(year).year}',\n",
    "                   yaxis = {\"range\":[-1,5],\"autorange\": False, \"showline\":False, \"visible\":True},\n",
    "                   xaxis = {\"range\":[-1,150],\"autorange\": False, \"showline\":False, \"visible\":False},\n",
    "                   plot_bgcolor= '#FFFFFF',\n",
    "                   updatemenus=[dict(type=\"buttons\",\n",
    "                                     buttons=[dict(label=\"▶️\",\n",
    "                                                   method=\"animate\",\n",
    "                                                   args=[None, {\"frame\": {\"duration\": 1200, \"redraw\": True},\n",
    "                                                                \"fromcurrent\": True}]),\n",
    "                                              {\n",
    "                \"args\": [[None], {\"frame\": {\"duration\": 0, \"redraw\": False},\n",
    "                                  \"mode\": \"immediate\",\n",
    "                                  \"transition\": {\"duration\": 0}}],\n",
    "                \"label\": \"⏸️\",\n",
    "                \"method\": \"animate\"\n",
    "            }\n",
    "                                             ]\n",
    "                                    )\n",
    "                               ]\n",
    "                  )\n",
    "\n",
    "fig = go.Figure(data=[trace], layout=layout, frames=frames)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text = re.sub(\"[^'’ \\w]|_\",\" \", \" \".join(df['text_clean'].values))\n",
    "words = [word for word in all_text.split() if len(word)>2]\n",
    "\n",
    "print(f\"Total characters written: {len(all_text):,.0f}\")\n",
    "print(f\"Number of words: ± {len(words):,.0f}\")\n",
    "print(f\"Number of unique words: ± {len(set(words)):,.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_en = stopwords.words('english')\n",
    "stopwords_sp = stopwords.words('spanish')\n",
    "sw = set(stopwords_en + stopwords_sp + list(stopwords_en_spacy) + list(stopwords_sp_spacy))\n",
    "sw.add(\"can't\")\n",
    "sw.add(\"like\")\n",
    "\n",
    "\n",
    "words_long2 = [word for word in words if len(word)>2]\n",
    "words_long3 = [word for word in words if len(word)>3]\n",
    "words_long4 = [word for word in words if len(word)>4]\n",
    "\n",
    "words_sw2 = [word for word in words_long2 if word.lower() not in sw]\n",
    "words_sw3 = [word for word in words_long3 if word.lower() not in sw]\n",
    "words_sw4 = [word for word in words_long4 if word.lower() not in sw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Top words used:\\n\")\n",
    "Counter(words_sw4).most_common()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gram = 2\n",
    "Counter(ngrams(words_sw2, n_gram)).most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = all_text\n",
    "wordcloud = WordCloud(width=1600, height=800, max_font_size=200, stopwords=sw,\n",
    "                      background_color='white', colormap='Dark2').generate(words)\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_frequent_words_visualization(df, col='text', title=\"\", color_palette=\"Dark2\"):\n",
    "        '''\n",
    "        input: data frame with tweets.\n",
    "        output: word cloud with most frequent words from column \"text\", i.e., the tweets themselves.\n",
    "        '''\n",
    "        words = \" \".join(sum(df[col].values,[]))\n",
    "        wordcloud = WordCloud(width=1600, height=800, max_font_size=200, \n",
    "                              background_color='white', colormap=color_palette).generate(words)\n",
    "        plt.figure(figsize=(15,10))\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.title(title)\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processed_text(text_clean):\n",
    "    text_clean = text_clean.replace(\"’\",\"'\")\n",
    "    text_clean = re.sub(\"[^'’ \\w]|_\",\" \", text_clean)\n",
    "    words = [word for word in text_clean.split() if (len(word)>2) and (word.lower() not in sw)]\n",
    "    return words\n",
    "\n",
    "df['processed_text'] = df['text_clean'].apply(lambda tc: processed_text(tc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_frequent_words_visualization(df[df['lang']=='pt'], 'processed_text', \"Portuguese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_frequent_words_visualization(df[df['lang']=='ru'], 'processed_text', \"Russian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_frequent_words_visualization(df[df['lang']=='fr'], 'processed_text', \"French\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_frequent_words_visualization(df[df['lang'].isin(['ru', 'pt', 'fr'])], 'processed_text', \"RU-PT-FR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for year in sorted(df.year.unique()):\n",
    "    mask = np.array(PIL.Image.open(\"twitter.png\"))\n",
    "    words = \" \".join(sum(df[df['year']==year]['processed_text'].values,[]))\n",
    "    wc = WordCloud(background_color=\"white\", mode=\"RGBA\", max_words=500, max_font_size=200, mask=mask).generate(words)\n",
    "    image_colors = ImageColorGenerator(mask)\n",
    "    plt.figure(figsize=(15,15));\n",
    "    plt.title(year);\n",
    "    plt.imshow(wc.recolor(color_func=image_colors), interpolation='bilinear');\n",
    "    plt.axis(\"off\");\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "englishStemmer=SnowballStemmer(\"english\")\n",
    "spanishStemmer=SnowballStemmer(\"spanish\")\n",
    "\n",
    "df_english = df[df['lang']=='en']\n",
    "df_spanish = df[df['lang']=='es']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc_top_n(text_processed_lemma, n):\n",
    "    d = dictionary.doc2bow(text_processed_lemma)\n",
    "    topics = ldamodel.get_document_topics(d)\n",
    "    try:\n",
    "        return topics[n][1]\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_processed_lemma(doc, stemmer):\n",
    "    doc = sum([re.sub(\"[^\\w]|_\",\" \",word).split() for word in doc],[])\n",
    "    doc = [word for word in doc if len(word)>2 and word.lower() not in sw]\n",
    "    if \"math\" in doc or \"maths\" in doc:\n",
    "        doc = [\"mathematics\" if (word==\"math\" or word==\"maths\") else word for word in doc]\n",
    "    pt = pos_tag(doc)\n",
    "    doc = [word for word, typ in pt if typ[0] in ['N']]\n",
    "    doc_stem = [stemmer.stem(w) for w in doc if len(w)>3]\n",
    "    if len(doc_stem)<5:\n",
    "        return None\n",
    "    return doc_stem\n",
    "    '''\n",
    "    doc_bg = []\n",
    "    for key,value in Counter(ngrams([word.lower() for word in doc],2)).items():\n",
    "        for i in range(value):\n",
    "            doc_bg.append(\" \".join(key))\n",
    "    doc = doc_stem + doc_bg\n",
    "    return doc\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_english['text_processed_lemma'] = df_english['processed_text'].apply(lambda doc: text_processed_lemma(doc, englishStemmer))\n",
    "df_english = df_english.dropna(subset=['text_processed_lemma'])\n",
    "df_english = df_english[df_english.index.isin(df_english['text_processed_lemma'].apply(tuple, 1).drop_duplicates().index)]\n",
    "\n",
    "df_spanish['text_processed_lemma'] = df_spanish['processed_text'].apply(lambda text: text_processed_lemma(text, spanishStemmer))\n",
    "df_spanish = df_spanish.dropna(subset=['text_processed_lemma'])\n",
    "df_spanish = df_spanish[df_spanish.index.isin(df_spanish['text_processed_lemma'].apply(tuple, 1).drop_duplicates().index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(list(df_english.text_processed_lemma.values))\n",
    "dictionary.filter_extremes(no_below=20, no_above=0.5)\n",
    "corpus = [dictionary.doc2bow(text) for text in list(df_english.text_processed_lemma.values)]\n",
    "\n",
    "NUM_TOPICS = 2\n",
    "ldamodel = LdaModel(corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes=30, alpha=0.01, eta=0.01)\n",
    "topics = ldamodel.print_topics(num_words=5)\n",
    "for topic in topics:\n",
    "    print(topic)\n",
    "    print()\n",
    "    \n",
    "lda_display = pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary, sort_topics=True)\n",
    "pyLDAvis.display(lda_display)\n",
    "\n",
    "# pyLDAvis.save_html(lda_display, 'lda.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(0,NUM_TOPICS):\n",
    "    top_name = f\"topic_{t}\"\n",
    "    df_english[top_name] = df_english['text_processed_lemma'].apply(lambda doc: get_doc_top_n(doc, t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for t in range(0,NUM_TOPICS):\n",
    "    print(f\"****************************** TOPIC {t} ******************************\")\n",
    "    topic = f\"topic_{t}\"\n",
    "    for i, tweet in enumerate(df_english.sort_values(topic, ascending=False)['text_clean'].head()):\n",
    "        print(f\"Tweet #{i+1}\")\n",
    "        print(tweet)\n",
    "        print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(list(df_spanish.text_processed_lemma.values))\n",
    "dictionary.filter_extremes(no_below=20, no_above=0.5)\n",
    "corpus = [dictionary.doc2bow(text) for text in list(df_spanish.text_processed_lemma.values)]\n",
    "\n",
    "NUM_TOPICS = 3\n",
    "ldamodel = LdaModel(corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes=30, alpha=0.01, eta=0.01)\n",
    "topics = ldamodel.print_topics(num_words=5)\n",
    "for topic in topics:\n",
    "    print(topic)\n",
    "    print()\n",
    "    \n",
    "lda_display = pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary, sort_topics=True)\n",
    "pyLDAvis.display(lda_display)\n",
    "\n",
    "# pyLDAvis.save_html(lda_display, 'lda.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(0,NUM_TOPICS):\n",
    "    top_name = f\"topic_{t}\"\n",
    "    df_spanish[top_name] = df_spanish['text_processed_lemma'].apply(lambda doc: get_doc_top_n(doc, t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for t in range(0,NUM_TOPICS):\n",
    "    print(f\"****************************** TOPIC {t} ******************************\")\n",
    "    topic = f\"topic_{t}\"\n",
    "    for i, tweet in enumerate(df_spanish.sort_values(topic, ascending=False)['text_clean'].head()):\n",
    "        print(f\"Tweet #{i+1}\")\n",
    "        print(tweet)\n",
    "        print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_processed_lemma_en(text_clean):\n",
    "    doc = processed_text(text_clean)\n",
    "    stemmer = englishStemmer\n",
    "    doc = sum([re.sub(\"[^\\w]|_\",\" \",word).split() for word in doc],[])\n",
    "    doc = [word for word in doc if len(word)>2 and word.lower() not in sw]\n",
    "    if \"math\" in doc or \"maths\" in doc:\n",
    "        doc = [\"mathematics\" if (word==\"math\" or word==\"maths\") else word for word in doc]\n",
    "    pt = pos_tag(doc)\n",
    "    doc = [word for word, typ in pt if typ[0] in ['N', 'V']]\n",
    "    doc_stem = [stemmer.stem(w) for w in doc if len(w)>3]\n",
    "    #if len(doc_stem)<5:\n",
    "    #    return None\n",
    "    return doc_stem\n",
    "    '''\n",
    "    doc_bg = []\n",
    "    for key,value in Counter(ngrams([word.lower() for word in doc],2)).items():\n",
    "        for i in range(value):\n",
    "            doc_bg.append(\" \".join(key))\n",
    "    doc = doc_stem + doc_bg\n",
    "    return doc\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_vectorizer = CountVectorizer(tokenizer = text_processed_lemma_en, min_df=10, stop_words=sw, ngram_range=(1, 2))\n",
    "features = token_vectorizer.fit_transform(df_english.text_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = []\n",
    "kinertia = []\n",
    "\n",
    "for k in range(1,20):\n",
    "    kmeans = KMeans(n_clusters=k).fit(features)\n",
    "    ks.append(k)\n",
    "    kinertia.append(kmeans.inertia_)\n",
    "    \n",
    "plt.plot(ks,kinertia, 'bx-');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_k = 17\n",
    "model = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "model.fit(features)\n",
    "\n",
    "print(\"Top terms per cluster:\")\n",
    "order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = token_vectorizer.get_feature_names()\n",
    "for i in range(true_k):\n",
    "    print(\"Cluster %d:\" % i),\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(' %s' % terms[ind]),\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_english['kmeans'] = model.labels_\n",
    "df_english.kmeans.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_processed_lemma_sp(text_clean):\n",
    "    doc = processed_text(text_clean)\n",
    "    stemmer = spanishStemmer\n",
    "    doc = sum([re.sub(\"[^\\w]|_\",\" \",word).split() for word in doc],[])\n",
    "    doc = [word for word in doc if len(word)>2 and word.lower() not in sw]\n",
    "    if \"math\" in doc or \"maths\" in doc:\n",
    "        doc = [\"mathematics\" if (word==\"math\" or word==\"maths\") else word for word in doc]\n",
    "    pt = pos_tag(doc)\n",
    "    doc = [word for word, typ in pt if typ[0] in ['N', 'V']]\n",
    "    doc_stem = [stemmer.stem(w) for w in doc if len(w)>3]\n",
    "    #if len(doc_stem)<5:\n",
    "    #    return None\n",
    "    return doc_stem\n",
    "    '''\n",
    "    doc_bg = []\n",
    "    for key,value in Counter(ngrams([word.lower() for word in doc],2)).items():\n",
    "        for i in range(value):\n",
    "            doc_bg.append(\" \".join(key))\n",
    "    doc = doc_stem + doc_bg\n",
    "    return doc\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_vectorizer = CountVectorizer(tokenizer = text_processed_lemma_sp, min_df=10, stop_words=sw, ngram_range=(1, 2))\n",
    "features = token_vectorizer.fit_transform(df_spanish.text_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = []\n",
    "kinertia = []\n",
    "\n",
    "for k in range(1,20):\n",
    "    kmeans = KMeans(n_clusters=k).fit(features)\n",
    "    ks.append(k)\n",
    "    kinertia.append(kmeans.inertia_)\n",
    "    \n",
    "plt.plot(ks,kinertia, 'bx-');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_k = 11\n",
    "model = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "model.fit(features)\n",
    "\n",
    "print(\"Top terms per cluster:\")\n",
    "order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = token_vectorizer.get_feature_names()\n",
    "for i in range(true_k):\n",
    "    print(\"Cluster %d:\" % i),\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(' %s' % terms[ind]),\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spanish['kmeans'] = model.labels_\n",
    "df_spanish.kmeans.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
